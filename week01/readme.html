<!DOCTYPE html><html><head><meta charset="utf-8"><style>body {
  width: 45em;
  border: 1px solid #ddd;
  outline: 1300px solid #fff;
  margin: 16px auto;
}

body .markdown-body
{
  padding: 30px;
}

@font-face {
  font-family: fontawesome-mini;
  src: url(data:font/woff;charset=utf-8;base64,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) format('woff');
}

@font-face {
  font-family: octicons-anchor;
  src: url(data:font/woff;charset=utf-8;base64,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) format('woff');
}

.markdown-body {
  font-family: sans-serif;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
  color: #333333;
  overflow: hidden;
  font-family: "Helvetica Neue", Helvetica, "Segoe UI", Arial, freesans, sans-serif;
  font-size: 16px;
  line-height: 1.6;
  word-wrap: break-word;
}

.markdown-body a {
  background: transparent;
}

.markdown-body a:active,
.markdown-body a:hover {
  outline: 0;
}

.markdown-body b,
.markdown-body strong {
  font-weight: bold;
}

.markdown-body mark {
  background: #ff0;
  color: #000;
  font-style: italic;
  font-weight: bold;
}

.markdown-body sub,
.markdown-body sup {
  font-size: 75%;
  line-height: 0;
  position: relative;
  vertical-align: baseline;
}
.markdown-body sup {
  top: -0.5em;
}
.markdown-body sub {
  bottom: -0.25em;
}

.markdown-body h1 {
  font-size: 2em;
  margin: 0.67em 0;
}

.markdown-body img {
  border: 0;
}

.markdown-body hr {
  -moz-box-sizing: content-box;
  box-sizing: content-box;
  height: 0;
}

.markdown-body pre {
  overflow: auto;
}

.markdown-body code,
.markdown-body kbd,
.markdown-body pre,
.markdown-body samp {
  font-family: monospace, monospace;
  font-size: 1em;
}

.markdown-body input {
  color: inherit;
  font: inherit;
  margin: 0;
}

.markdown-body html input[disabled] {
  cursor: default;
}

.markdown-body input {
  line-height: normal;
}

.markdown-body input[type="checkbox"] {
  box-sizing: border-box;
  padding: 0;
}

.markdown-body table {
  border-collapse: collapse;
  border-spacing: 0;
}

.markdown-body td,
.markdown-body th {
  padding: 0;
}

.markdown-body .codehilitetable {
  border: 0;
  border-spacing: 0;
}

.markdown-body .codehilitetable tr {
  border: 0;
}

.markdown-body .codehilitetable pre,
.markdown-body .codehilitetable div.codehilite {
  margin: 0;
}

.markdown-body .linenos,
.markdown-body .code,
.markdown-body .codehilitetable td {
  border: 0;
  padding: 0;
}

.markdown-body td:not(.linenos) .linenodiv {
  padding: 0 !important;
}

.markdown-body .code {
  width: 100%;
}

.markdown-body .linenos div pre,
.markdown-body .linenodiv pre,
.markdown-body .linenodiv {
  border: 0;
  -webkit-border-radius: 0;
  -moz-border-radius: 0;
  border-radius: 0;
  -webkit-border-top-left-radius: 3px;
  -webkit-border-bottom-left-radius: 3px;
  -moz-border-radius-topleft: 3px;
  -moz-border-radius-bottomleft: 3px;
  border-top-left-radius: 3px;
  border-bottom-left-radius: 3px;
}

.markdown-body .code div pre,
.markdown-body .code div {
  border: 0;
  -webkit-border-radius: 0;
  -moz-border-radius: 0;
  border-radius: 0;
  -webkit-border-top-right-radius: 3px;
  -webkit-border-bottom-right-radius: 3px;
  -moz-border-radius-topright: 3px;
  -moz-border-radius-bottomright: 3px;
  border-top-right-radius: 3px;
  border-bottom-right-radius: 3px;
}

.markdown-body * {
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}

.markdown-body input {
  font: 13px Helvetica, arial, freesans, clean, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol";
  line-height: 1.4;
}

.markdown-body a {
  color: #4183c4;
  text-decoration: none;
}

.markdown-body a:hover,
.markdown-body a:focus,
.markdown-body a:active {
  text-decoration: underline;
}

.markdown-body hr {
  height: 0;
  margin: 15px 0;
  overflow: hidden;
  background: transparent;
  border: 0;
  border-bottom: 1px solid #ddd;
}

.markdown-body hr:before,
.markdown-body hr:after {
  display: table;
  content: " ";
}

.markdown-body hr:after {
  clear: both;
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  margin-top: 15px;
  margin-bottom: 15px;
  line-height: 1.1;
}

.markdown-body h1 {
  font-size: 30px;
}

.markdown-body h2 {
  font-size: 21px;
}

.markdown-body h3 {
  font-size: 16px;
}

.markdown-body h4 {
  font-size: 14px;
}

.markdown-body h5 {
  font-size: 12px;
}

.markdown-body h6 {
  font-size: 11px;
}

.markdown-body blockquote {
  margin: 0;
}

.markdown-body ul,
.markdown-body ol {
  padding: 0;
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body ol ol,
.markdown-body ul ol {
  list-style-type: lower-roman;
}

.markdown-body ul ul ol,
.markdown-body ul ol ol,
.markdown-body ol ul ol,
.markdown-body ol ol ol {
  list-style-type: lower-alpha;
}

.markdown-body dd {
  margin-left: 0;
}

.markdown-body code,
.markdown-body pre,
.markdown-body samp {
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  font-size: 12px;
}

.markdown-body pre {
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body kbd {
  background-color: #e7e7e7;
  background-image: -moz-linear-gradient(#fefefe, #e7e7e7);
  background-image: -webkit-linear-gradient(#fefefe, #e7e7e7);
  background-image: linear-gradient(#fefefe, #e7e7e7);
  background-repeat: repeat-x;
  border-radius: 2px;
  border: 1px solid #cfcfcf;
  color: #000;
  padding: 3px 5px;
  line-height: 10px;
  font: 11px Consolas, "Liberation Mono", Menlo, Courier, monospace;
  display: inline-block;
}

.markdown-body>*:first-child {
  margin-top: 0 !important;
}

.markdown-body>*:last-child {
  margin-bottom: 0 !important;
}

.markdown-body .headeranchor-link {
  position: absolute;
  top: 0;
  bottom: 0;
  left: 0;
  display: block;
  padding-right: 6px;
  padding-left: 30px;
  margin-left: -30px;
}

.markdown-body .headeranchor-link:focus {
  outline: none;
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  position: relative;
  margin-top: 1em;
  margin-bottom: 16px;
  font-weight: bold;
  line-height: 1.4;
}

.markdown-body h1 .headeranchor,
.markdown-body h2 .headeranchor,
.markdown-body h3 .headeranchor,
.markdown-body h4 .headeranchor,
.markdown-body h5 .headeranchor,
.markdown-body h6 .headeranchor {
  display: none;
  color: #000;
  vertical-align: middle;
}

.markdown-body h1:hover .headeranchor-link,
.markdown-body h2:hover .headeranchor-link,
.markdown-body h3:hover .headeranchor-link,
.markdown-body h4:hover .headeranchor-link,
.markdown-body h5:hover .headeranchor-link,
.markdown-body h6:hover .headeranchor-link {
  height: 1em;
  padding-left: 8px;
  margin-left: -30px;
  line-height: 1;
  text-decoration: none;
}

.markdown-body h1:hover .headeranchor-link .headeranchor,
.markdown-body h2:hover .headeranchor-link .headeranchor,
.markdown-body h3:hover .headeranchor-link .headeranchor,
.markdown-body h4:hover .headeranchor-link .headeranchor,
.markdown-body h5:hover .headeranchor-link .headeranchor,
.markdown-body h6:hover .headeranchor-link .headeranchor {
  display: inline-block;
}

.markdown-body h1 {
  padding-bottom: 0.3em;
  font-size: 2.25em;
  line-height: 1.2;
  border-bottom: 1px solid #eee;
}

.markdown-body h2 {
  padding-bottom: 0.3em;
  font-size: 1.75em;
  line-height: 1.225;
  border-bottom: 1px solid #eee;
}

.markdown-body h3 {
  font-size: 1.5em;
  line-height: 1.43;
}

.markdown-body h4 {
  font-size: 1.25em;
}

.markdown-body h5 {
  font-size: 1em;
}

.markdown-body h6 {
  font-size: 1em;
  color: #777;
}

.markdown-body p,
.markdown-body blockquote,
.markdown-body ul,
.markdown-body ol,
.markdown-body dl,
.markdown-body table,
.markdown-body pre,
.markdown-body .admonition {
  margin-top: 0;
  margin-bottom: 16px;
}

.markdown-body hr {
  height: 4px;
  padding: 0;
  margin: 16px 0;
  background-color: #e7e7e7;
  border: 0 none;
}

.markdown-body ul,
.markdown-body ol {
  padding-left: 2em;
}

.markdown-body ul ul,
.markdown-body ul ol,
.markdown-body ol ol,
.markdown-body ol ul {
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body li>p {
  margin-top: 16px;
}

.markdown-body dl {
  padding: 0;
}

.markdown-body dl dt {
  padding: 0;
  margin-top: 16px;
  font-size: 1em;
  font-style: italic;
  font-weight: bold;
}

.markdown-body dl dd {
  padding: 0 16px;
  margin-bottom: 16px;
}

.markdown-body blockquote {
  padding: 0 15px;
  color: #777;
  border-left: 4px solid #ddd;
}

.markdown-body blockquote>:first-child {
  margin-top: 0;
}

.markdown-body blockquote>:last-child {
  margin-bottom: 0;
}

.markdown-body table {
  display: block;
  width: 100%;
  overflow: auto;
  word-break: normal;
  word-break: keep-all;
}

.markdown-body table th {
  font-weight: bold;
}

.markdown-body table th,
.markdown-body table td {
  padding: 6px 13px;
  border: 1px solid #ddd;
}

.markdown-body table tr {
  background-color: #fff;
  border-top: 1px solid #ccc;
}

.markdown-body table tr:nth-child(2n) {
  background-color: #f8f8f8;
}

.markdown-body img {
  max-width: 100%;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}

.markdown-body code,
.markdown-body samp {
  padding: 0;
  padding-top: 0.2em;
  padding-bottom: 0.2em;
  margin: 0;
  font-size: 85%;
  background-color: rgba(0,0,0,0.04);
  border-radius: 3px;
}

.markdown-body code:before,
.markdown-body code:after {
  letter-spacing: -0.2em;
  content: "\00a0";
}

.markdown-body pre>code {
  padding: 0;
  margin: 0;
  font-size: 100%;
  word-break: normal;
  white-space: pre;
  background: transparent;
  border: 0;
}

.markdown-body .codehilite {
  margin-bottom: 16px;
}

.markdown-body .codehilite pre,
.markdown-body pre {
  padding: 16px;
  overflow: auto;
  font-size: 85%;
  line-height: 1.45;
  background-color: #f7f7f7;
  border-radius: 3px;
}

.markdown-body .codehilite pre {
  margin-bottom: 0;
  word-break: normal;
}

.markdown-body pre {
  word-wrap: normal;
}

.markdown-body pre code {
  display: inline;
  max-width: initial;
  padding: 0;
  margin: 0;
  overflow: initial;
  line-height: inherit;
  word-wrap: normal;
  background-color: transparent;
  border: 0;
}

.markdown-body pre code:before,
.markdown-body pre code:after {
  content: normal;
}

/* Admonition */
.markdown-body .admonition {
  -webkit-border-radius: 3px;
  -moz-border-radius: 3px;
  position: relative;
  border-radius: 3px;
  border: 1px solid #e0e0e0;
  border-left: 6px solid #333;
  padding: 10px 10px 10px 30px;
}

.markdown-body .admonition table {
  color: #333;
}

.markdown-body .admonition p {
  padding: 0;
}

.markdown-body .admonition-title {
  font-weight: bold;
  margin: 0;
}

.markdown-body .admonition>.admonition-title {
  color: #333;
}

.markdown-body .attention>.admonition-title {
  color: #a6d796;
}

.markdown-body .caution>.admonition-title {
  color: #d7a796;
}

.markdown-body .hint>.admonition-title {
  color: #96c6d7;
}

.markdown-body .danger>.admonition-title {
  color: #c25f77;
}

.markdown-body .question>.admonition-title {
  color: #96a6d7;
}

.markdown-body .note>.admonition-title {
  color: #d7c896;
}

.markdown-body .admonition:before,
.markdown-body .attention:before,
.markdown-body .caution:before,
.markdown-body .hint:before,
.markdown-body .danger:before,
.markdown-body .question:before,
.markdown-body .note:before {
  font: normal normal 16px fontawesome-mini;
  -moz-osx-font-smoothing: grayscale;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  line-height: 1.5;
  color: #333;
  position: absolute;
  left: 0;
  top: 0;
  padding-top: 10px;
  padding-left: 10px;
}

.markdown-body .admonition:before {
  content: "\f056\00a0";
  color: 333;
}

.markdown-body .attention:before {
  content: "\f058\00a0";
  color: #a6d796;
}

.markdown-body .caution:before {
  content: "\f06a\00a0";
  color: #d7a796;
}

.markdown-body .hint:before {
  content: "\f05a\00a0";
  color: #96c6d7;
}

.markdown-body .danger:before {
  content: "\f057\00a0";
  color: #c25f77;
}

.markdown-body .question:before {
  content: "\f059\00a0";
  color: #96a6d7;
}

.markdown-body .note:before {
  content: "\f040\00a0";
  color: #d7c896;
}

.markdown-body .admonition::after {
  content: normal;
}

.markdown-body .attention {
  border-left: 6px solid #a6d796;
}

.markdown-body .caution {
  border-left: 6px solid #d7a796;
}

.markdown-body .hint {
  border-left: 6px solid #96c6d7;
}

.markdown-body .danger {
  border-left: 6px solid #c25f77;
}

.markdown-body .question {
  border-left: 6px solid #96a6d7;
}

.markdown-body .note {
  border-left: 6px solid #d7c896;
}

.markdown-body .admonition>*:first-child {
  margin-top: 0 !important;
}

.markdown-body .admonition>*:last-child {
  margin-bottom: 0 !important;
}

/* progress bar*/
.markdown-body .progress {
  display: block;
  width: 300px;
  margin: 10px 0;
  height: 24px;
  -webkit-border-radius: 3px;
  -moz-border-radius: 3px;
  border-radius: 3px;
  background-color: #ededed;
  position: relative;
  box-shadow: inset -1px 1px 3px rgba(0, 0, 0, .1);
}

.markdown-body .progress-label {
  position: absolute;
  text-align: center;
  font-weight: bold;
  width: 100%; margin: 0;
  line-height: 24px;
  color: #333;
  text-shadow: 1px 1px 0 #fefefe, -1px -1px 0 #fefefe, -1px 1px 0 #fefefe, 1px -1px 0 #fefefe, 0 1px 0 #fefefe, 0 -1px 0 #fefefe, 1px 0 0 #fefefe, -1px 0 0 #fefefe, 1px 1px 2px #000;
  -webkit-font-smoothing: antialiased !important;
  white-space: nowrap;
  overflow: hidden;
}

.markdown-body .progress-bar {
  height: 24px;
  float: left;
  -webkit-border-radius: 3px;
  -moz-border-radius: 3px;
  border-radius: 3px;
  background-color: #96c6d7;
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, .5), inset 0 -1px 0 rgba(0, 0, 0, .1);
  background-size: 30px 30px;
  background-image: -webkit-linear-gradient(
    135deg, rgba(255, 255, 255, .4) 27%,
    transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%,
    transparent 77%, transparent
  );
  background-image: -moz-linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
  background-image: -ms-linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
  background-image: -o-linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
  background-image: linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
}

.markdown-body .progress-100plus .progress-bar {
  background-color: #a6d796;
}

.markdown-body .progress-80plus .progress-bar {
  background-color: #c6d796;
}

.markdown-body .progress-60plus .progress-bar {
  background-color: #d7c896;
}

.markdown-body .progress-40plus .progress-bar {
  background-color: #d7a796;
}

.markdown-body .progress-20plus .progress-bar {
  background-color: #d796a6;
}

.markdown-body .progress-0plus .progress-bar {
  background-color: #c25f77;
}

.markdown-body .candystripe-animate .progress-bar{
  -webkit-animation: animate-stripes 3s linear infinite;
  -moz-animation: animate-stripes 3s linear infinite;
  animation: animate-stripes 3s linear infinite;
}

@-webkit-keyframes animate-stripes {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 60px 0;
  }
}

@-moz-keyframes animate-stripes {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 60px 0;
  }
}

@keyframes animate-stripes {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 60px 0;
  }
}

.markdown-body .gloss .progress-bar {
  box-shadow:
    inset 0 4px 12px rgba(255, 255, 255, .7),
    inset 0 -12px 0 rgba(0, 0, 0, .05);
}

/* Multimarkdown Critic Blocks */
.markdown-body .critic_mark {
  background: #ff0;
}

.markdown-body .critic_delete {
  color: #c82829;
  text-decoration: line-through;
}

.markdown-body .critic_insert {
  color: #718c00 ;
  text-decoration: underline;
}

.markdown-body .critic_comment {
  color: #8e908c;
  font-style: italic;
}

.markdown-body .headeranchor {
  font: normal normal 16px octicons-anchor;
  line-height: 1;
  display: inline-block;
  text-decoration: none;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.headeranchor:before {
  content: '\f05c';
}

.markdown-body .task-list-item {
  list-style-type: none;
}

.markdown-body .task-list-item+.task-list-item {
  margin-top: 3px;
}

.markdown-body .task-list-item input {
  margin: 0 4px 0.25em -20px;
  vertical-align: middle;
}

/* Media */
@media only screen and (min-width: 480px) {
  .markdown-body {
    font-size:14px;
  }
}

@media only screen and (min-width: 768px) {
  .markdown-body {
    font-size:16px;
  }
}

@media print {
  .markdown-body * {
    background: transparent !important;
    color: black !important;
    filter:none !important;
    -ms-filter: none !important;
  }

  .markdown-body {
    font-size:12pt;
    max-width:100%;
    outline:none;
    border: 0;
  }

  .markdown-body a,
  .markdown-body a:visited {
    text-decoration: underline;
  }

  .markdown-body .headeranchor-link {
    display: none;
  }

  .markdown-body a[href]:after {
    content: " (" attr(href) ")";
  }

  .markdown-body abbr[title]:after {
    content: " (" attr(title) ")";
  }

  .markdown-body .ir a:after,
  .markdown-body a[href^="javascript:"]:after,
  .markdown-body a[href^="#"]:after {
    content: "";
  }

  .markdown-body pre {
    white-space: pre;
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  .markdown-body pre,
  .markdown-body blockquote {
    border: 1px solid #999;
    padding-right: 1em;
    page-break-inside: avoid;
  }

  .markdown-body .progress,
  .markdown-body .progress-bar {
    -moz-box-shadow: none;
    -webkit-box-shadow: none;
    box-shadow: none;
  }

  .markdown-body .progress {
    border: 1px solid #ddd;
  }

  .markdown-body .progress-bar {
    height: 22px;
    border-right: 1px solid #ddd;
  }

  .markdown-body tr,
  .markdown-body img {
    page-break-inside: avoid;
  }

  .markdown-body img {
    max-width: 100% !important;
  }

  .markdown-body p,
  .markdown-body h2,
  .markdown-body h3 {
    orphans: 3;
    widows: 3;
  }

  .markdown-body h2,
  .markdown-body h3 {
    page-break-after: avoid;
  }
}
</style><style>/*github*/
.codehilite {background-color:#fff;color:#333333;}
.codehilite .hll {background-color:#ffffcc;}
.codehilite .c{color:#999988;font-style:italic}
.codehilite .err{color:#a61717;background-color:#e3d2d2}
.codehilite .k{font-weight:bold}
.codehilite .o{font-weight:bold}
.codehilite .cm{color:#999988;font-style:italic}
.codehilite .cp{color:#999999;font-weight:bold}
.codehilite .c1{color:#999988;font-style:italic}
.codehilite .cs{color:#999999;font-weight:bold;font-style:italic}
.codehilite .gd{color:#000000;background-color:#ffdddd}
.codehilite .ge{font-style:italic}
.codehilite .gr{color:#aa0000}
.codehilite .gh{color:#999999}
.codehilite .gi{color:#000000;background-color:#ddffdd}
.codehilite .go{color:#888888}
.codehilite .gp{color:#555555}
.codehilite .gs{font-weight:bold}
.codehilite .gu{color:#800080;font-weight:bold}
.codehilite .gt{color:#aa0000}
.codehilite .kc{font-weight:bold}
.codehilite .kd{font-weight:bold}
.codehilite .kn{font-weight:bold}
.codehilite .kp{font-weight:bold}
.codehilite .kr{font-weight:bold}
.codehilite .kt{color:#445588;font-weight:bold}
.codehilite .m{color:#009999}
.codehilite .s{color:#dd1144}
.codehilite .n{color:#333333}
.codehilite .na{color:teal}
.codehilite .nb{color:#0086b3}
.codehilite .nc{color:#445588;font-weight:bold}
.codehilite .no{color:teal}
.codehilite .ni{color:purple}
.codehilite .ne{color:#990000;font-weight:bold}
.codehilite .nf{color:#990000;font-weight:bold}
.codehilite .nn{color:#555555}
.codehilite .nt{color:navy}
.codehilite .nv{color:teal}
.codehilite .ow{font-weight:bold}
.codehilite .w{color:#bbbbbb}
.codehilite .mf{color:#009999}
.codehilite .mh{color:#009999}
.codehilite .mi{color:#009999}
.codehilite .mo{color:#009999}
.codehilite .sb{color:#dd1144}
.codehilite .sc{color:#dd1144}
.codehilite .sd{color:#dd1144}
.codehilite .s2{color:#dd1144}
.codehilite .se{color:#dd1144}
.codehilite .sh{color:#dd1144}
.codehilite .si{color:#dd1144}
.codehilite .sx{color:#dd1144}
.codehilite .sr{color:#009926}
.codehilite .s1{color:#dd1144}
.codehilite .ss{color:#990073}
.codehilite .bp{color:#999999}
.codehilite .vc{color:teal}
.codehilite .vg{color:teal}
.codehilite .vi{color:teal}
.codehilite .il{color:#009999}
.codehilite .gc{color:#999;background-color:#EAF2F5}
</style><title>readme</title></head><body><article class="markdown-body"><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: { inlineMath: [['$','$'], ['$ ',' $']], processClass: 'math', processEscapes: true },
        'HTML-CSS': { linebreaks: { automatic: true } },
        SVG: { linebreaks: { automatic: true } }
        });
</script>

<script src="https://mathjax.cnblogs.com/2_7_2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<h1 id="_1"><a name="user-content-_1" href="#_1" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>第一周机器学习</h1>
<blockquote>
<div class="codehilite"><pre>本文公式显示需要使用Mathjax，然后令人悲伤的是github不支持Mathjax
您可以将这篇md文件pull下来，使用您本地的markdown解析器解析
没有必要在公示显示上浪费时间，您也可以下载我本地生成的html用浏览器打开即可
或者您也可以下载我上传到github上的pdf
</pre></div>


<p><em><a href="https://github.com/mathjax/MathJax">Mathjax开源项目地址</a></em></p>
</blockquote>
<h2 id="_2"><a name="user-content-_2" href="#_2" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>绪论</h2>
<h3 id="_3"><a name="user-content-_3" href="#_3" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>机器学习简介</h3>
<p>机器学习是一种将无序数据转换为价值的方法。<br />
机器学习的价值-从数据中抽取规律，并用来预测未来</p>
<h3 id="_4"><a name="user-content-_4" href="#_4" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>机器学习应用举例</h3>
<ul>
<li>分类问题-图像识别、垃圾邮件识别</li>
<li>回归问题-股价预测、房价预测</li>
<li>排序问题-点击率预估、推荐</li>
<li>生成问题-图像生成、图像风格转换、图像文字描述生成</li>
</ul>
<h3 id="_5"><a name="user-content-_5" href="#_5" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>机器学习的应用流程</h3>
<p><img alt="1555427506968" src="/home/thb/Documents/PyCode/machine_learning/week01/assets/1555427506968.png" /></p>
<h3 id="_6"><a name="user-content-_6" href="#_6" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>机器学习岗位职责</h3>
<ul>
<li>数据处理(采集+去噪)</li>
<li>模型训练(特征+模型)</li>
<li>模型评估与优化(MSE、F1-score、AUC+调参)</li>
<li>模型应用(A/B测试)</li>
</ul>
<h2 id="linear-regression"><a name="user-content-linear-regression" href="#linear-regression" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>线性回归(Linear regression)</h2>
<p>cost function:<br />
$$<br />
J(\theta) = \frac{1}{2}\sum_{i=1}^{m}(y^{(i)}-\theta^T x^{(i)})^2<br />
$$<br />
来源于假设误差 $  \varepsilon_i  $ 服从正态分布，然后对参数$  \theta  $进行极大似然估计，经过运算后得出$  J(\theta)  $取最小时，似然函数最大，从而推出这个式子。<br />
此外，对这个$  J(\theta)  $求偏导，令其偏导数为0（这里涉及到矩阵偏导数计算），即可得到正规方程(normal equation)。</p>
<h2 id="_7"><a name="user-content-_7" href="#_7" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>梯度下降法</h2>
<h3 id="_8"><a name="user-content-_8" href="#_8" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>分类</h3>
<ul>
<li>mini-batch</li>
<li>batch</li>
<li>random</li>
<li>SGD(动量梯度下降，有助于解决局部最值和鞍点问题)</li>
</ul>
<h3 id="code"><a name="user-content-code" href="#code" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Code</h3>
<p>参考代码，自己就一些细节进行优化<br />
(<a href="https://www.cnblogs.com/focusonepoint/p/6394339.html">https://www.cnblogs.com/focusonepoint/p/6394339.html</a>)</p>
<p><div class="codehilite"><pre><span class="ch">#!/usr/bin/python</span>
<span class="c1"># -*- coding: UTF-8 -*-</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">linalg</span>



<span class="k">def</span> <span class="nf">gradientDescent</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">maxIteration</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    使用批处理梯度下降算法计算theta</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="c1"># 得到x的转置</span>
    <span class="c1"># 即 x的第一行为x1 第二行为x2 第三行全部初始化为1</span>
    <span class="n">xTrains</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>
    <span class="c1"># theta 是一个列向量</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">maxIteration</span><span class="p">):</span>
        <span class="c1"># x矩阵(10*3)与theta(3*1)矩阵相乘</span>
        <span class="c1"># hypothesis(i) = x1(i)*theta1(i) + x2(i)*theta2(2) + 1*theta0(i)</span>
        <span class="n">hypothesis</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>
        <span class="c1"># 作差</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">hypothesis</span> <span class="o">-</span> <span class="n">y</span>
        <span class="c1"># 当loss的范数在我们的误差允许范围内 就停止循环</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-5</span><span class="p">):</span>
            <span class="k">break</span>
        <span class="c1"># xTrains (3*10) * loss(10*1) = gradient(3*1)</span>
        <span class="c1"># 计算代价函数</span>
        <span class="n">gradient</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xTrains</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">gradient</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&#39;the number of iteration is </span><span class="si">%d</span><span class="s">&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">);</span>
    <span class="k">return</span> <span class="n">theta</span>


<span class="c1"># define the prepared 训练集</span>
<span class="c1"># the meaning of column : x1,x2,y</span>
<span class="n">dataSet</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">1.1</span><span class="p">,</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">2.5</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">1.3</span><span class="p">,</span><span class="mf">1.9</span><span class="p">,</span><span class="mf">3.2</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">2.3</span><span class="p">,</span><span class="mf">3.9</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">1.7</span><span class="p">,</span><span class="mf">2.7</span><span class="p">,</span><span class="mf">4.6</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">1.9</span><span class="p">,</span><span class="mf">3.1</span><span class="p">,</span><span class="mf">5.3</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">2.1</span><span class="p">,</span><span class="mf">3.5</span><span class="p">,</span><span class="mf">6.0</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">2.3</span><span class="p">,</span><span class="mf">3.9</span><span class="p">,</span><span class="mf">6.7</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">2.5</span><span class="p">,</span><span class="mf">4.3</span><span class="p">,</span><span class="mf">7.4</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">2.7</span><span class="p">,</span><span class="mf">4.7</span><span class="p">,</span><span class="mf">8.1</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">2.9</span><span class="p">,</span><span class="mf">5.1</span><span class="p">,</span><span class="mf">8.8</span><span class="p">],</span>
<span class="p">])</span>


<span class="c1"># print(dataSet)</span>
<span class="n">m</span><span class="p">,</span><span class="n">n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">dataSet</span><span class="p">)</span>
<span class="c1"># print(m,n)</span>
<span class="n">trainData</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">m</span><span class="p">,</span><span class="n">n</span><span class="p">))</span>
<span class="c1"># 截取dataSet的前N-1列</span>
<span class="n">trainData</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">dataSet</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="c1"># 获取dataSet的最后一列</span>
<span class="n">trainLabel</span> <span class="o">=</span> <span class="n">dataSet</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># print(m,n)</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="c1"># print(theta)</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="c1"># the max time of iteration 这个值定义的尽量大(考虑计算机的性能)</span>
<span class="n">maxIteration</span> <span class="o">=</span> <span class="mi">10000000</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">gradientDescent</span><span class="p">(</span><span class="n">trainData</span><span class="p">,</span> <span class="n">trainLabel</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">maxIteration</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">&#39;thec value of theta is:&#39;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>


<span class="c1"># a test for the algorithm</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">3.1</span><span class="p">,</span> <span class="mf">5.5</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">3.3</span><span class="p">,</span> <span class="mf">5.9</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">3.5</span><span class="p">,</span> <span class="mf">6.3</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">3.7</span><span class="p">,</span> <span class="mf">6.7</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">3.9</span><span class="p">,</span> <span class="mf">7.1</span><span class="p">]</span>
<span class="p">])</span>


<span class="c1"># define a predict function used to test</span>
<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">):</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">xTest</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">xTest</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
    <span class="n">yPre</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xTest</span><span class="p">,</span><span class="n">theta</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">yPre</span>

<span class="k">print</span><span class="p">(</span><span class="s">&#39;the predicted value is&#39;</span><span class="p">)</span>
<span class="n">yP</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">yP</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
<br />
运行结果<br />
<div class="codehilite"><pre>the number of iteration is 114575
thec value of theta is:
[ 0.71  1.39 -0.38]
the predicted value is
[ 9.5 10.2 10.9 11.6 12.3]
[Finished in 2.2s]
</pre></div>
</p>
<h2 id="_9"><a name="user-content-_9" href="#_9" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>优化技巧</h2>
<ul>
<li>Feature Scaling（特征缩放）<ul>
<li>归一化<ol>
<li>线性归一化<ul>
<li>$  {x}&rsquo; = \frac{x - \min(x)}{\max(x) - \min(x)}  $</li>
</ul>
</li>
<li>标准差归一化<ul>
<li>$  x^* = \frac{x- \overline x}{s}  $</li>
</ul>
</li>
<li>非线性归一化</li>
</ol>
</li>
</ul>
</li>
<li>多项式回归<ul>
<li>$ h_\theta(x) = \theta_0 + \theta_1x + \theta_2x^2 $</li>
<li>$ h_\theta(x) = \theta_0 + \theta_1x + \theta_2x^2 + \theta_3x^3 $</li>
<li>$ h_\theta(x) = \theta_0 + \theta_1x + \theta_2\sqrt{x} $</li>
<li>上面的举例只是为了说明，x<sub>i</sub>的取值可以不是x的一次多项式，但是这里要注意的是特征缩放在这里显得尤为重要</li>
</ul>
</li>
<li>α选取技巧<ul>
<li>如果J（θ)的值随着θ的取值单调递增或者出现震荡，那么α应该选的小一点</li>
</ul>
</li>
</ul>
<h2 id="normal-equation"><a name="user-content-normal-equation" href="#normal-equation" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Normal Equation（正规方程法）</h2>
<h3 id="_10"><a name="user-content-_10" href="#_10" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>思想</h3>
<p>$$<br />
J_\theta(x) = \frac{1}{2m}\sum_{i=1}^m{(h_\theta(x)-y)^2} \\<br />
= a\theta^2 + b\theta + c<br />
$$</p>
<p>微积分思想：求导后令导数为零解方程可以求出极值点θ<br />
对于θ是一个n维向量的情况，可以利用多元函数取极值的必要条件，即偏导数为0</p>
<h3 id="_11"><a name="user-content-_11" href="#_11" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>结论</h3>
<p>$$<br />
\theta = (X^TX)^{-1}X^Ty<br />
$$</p>
<h3 id="note"><a name="user-content-note" href="#note" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Note</h3>
<ol>
<li>No need to do feature scaling</li>
<li>只适用于线性模型，不适合逻辑回归模型等其他模型</li>
<li>the pseudo inverse of matrix<ul>
<li>redundant features (x中存在线性相关的量)</li>
<li>too many features (eg. m &lt;= n 数据个数小于特征参数)</li>
</ul>
</li>
</ol>
<h3 id="code_1"><a name="user-content-code_1" href="#code_1" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Code</h3>
<p>这里我使用上一个梯度下降法的例子作为对比,采用相同的数据对比运行结果</p>
<p><div class="codehilite"><pre><span class="ch">#!/usr/bin/python</span>
<span class="c1"># -*- coding: UTF-8 -*-</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">normalEqation</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    使用正规方程法算法计算theta</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="c1"># 得到x的转置</span>
    <span class="n">xTrains</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>
    <span class="n">m</span><span class="p">,</span><span class="n">n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="c1"># theta 为 n维列向量</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xTrains</span><span class="p">,</span><span class="n">x</span><span class="p">))</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span><span class="n">xTrains</span><span class="p">)</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">theta</span>


<span class="c1"># define the prepared 训练集</span>
<span class="c1"># the meaning of column : x1,x2,y</span>
<span class="n">dataSet</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">1.1</span><span class="p">,</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">2.5</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">1.3</span><span class="p">,</span><span class="mf">1.9</span><span class="p">,</span><span class="mf">3.2</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">2.3</span><span class="p">,</span><span class="mf">3.9</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">1.7</span><span class="p">,</span><span class="mf">2.7</span><span class="p">,</span><span class="mf">4.6</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">1.9</span><span class="p">,</span><span class="mf">3.1</span><span class="p">,</span><span class="mf">5.3</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">2.1</span><span class="p">,</span><span class="mf">3.5</span><span class="p">,</span><span class="mf">6.0</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">2.3</span><span class="p">,</span><span class="mf">3.9</span><span class="p">,</span><span class="mf">6.7</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">2.5</span><span class="p">,</span><span class="mf">4.3</span><span class="p">,</span><span class="mf">7.4</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">2.7</span><span class="p">,</span><span class="mf">4.7</span><span class="p">,</span><span class="mf">8.1</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">2.9</span><span class="p">,</span><span class="mf">5.1</span><span class="p">,</span><span class="mf">8.8</span><span class="p">],</span>
<span class="p">])</span>


<span class="c1"># print(dataSet)</span>
<span class="n">m</span><span class="p">,</span><span class="n">n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">dataSet</span><span class="p">)</span>
<span class="c1"># print(m,n)</span>
<span class="n">trainData</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">m</span><span class="p">,</span><span class="n">n</span><span class="p">))</span>
<span class="c1"># 截取dataSet的前N-1列</span>
<span class="n">trainData</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">dataSet</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="c1"># 获取dataSet的最后一列</span>
<span class="n">trainLabel</span> <span class="o">=</span> <span class="n">dataSet</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>



<span class="n">theta</span> <span class="o">=</span> <span class="n">normalEqation</span><span class="p">(</span><span class="n">trainData</span><span class="p">,</span> <span class="n">trainLabel</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">&#39;thec value of theta is:&#39;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>


<span class="c1"># a test for the algorithm</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">3.1</span><span class="p">,</span> <span class="mf">5.5</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">3.3</span><span class="p">,</span> <span class="mf">5.9</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">3.5</span><span class="p">,</span> <span class="mf">6.3</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">3.7</span><span class="p">,</span> <span class="mf">6.7</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">3.9</span><span class="p">,</span> <span class="mf">7.1</span><span class="p">]</span>
<span class="p">])</span>


<span class="c1"># define a predict function used to test</span>
<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">):</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">xTest</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">xTest</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
    <span class="n">yPre</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xTest</span><span class="p">,</span><span class="n">theta</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">yPre</span>

<span class="k">print</span><span class="p">(</span><span class="s">&#39;the predicted value is&#39;</span><span class="p">)</span>
<span class="n">yP</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">yP</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
<br />
运行结果：<br />
<div class="codehilite"><pre>thec value of theta is:
[ 0.61  1.45 -0.34]
the predicted value is
[ 9.5 10.2 10.9 11.6 12.3]
[Finished in 0.2s]
</pre></div>
</p>
<p><strong>由此可以知道，在特征矩阵维度不是太大情况下，对于线性回归模型，normal equation 是一个优先选用的方法。</strong></p>
<h2 id="logistic-regression"><a name="user-content-logistic-regression" href="#logistic-regression" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Logistic Regression</h2>
<h3 id="logistic-function"><a name="user-content-logistic-function" href="#logistic-function" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>logistic function</h3>
<p>由于线性回归的假设函数不再适用于分类问题，因此我们需要一个函数来应用于分类问题的拟合。<br />
一般来说，回归不用在分类问题上，因为回归是连续型模型，而且受噪声影响比较大。如果非要应用进入，可以使用logistic回归。</p>
<p>我们可以使用logistic regression解决分类问题，Logistic回归是二分类任务的首选方法，下面讨论二分类的问题。<br />
$$<br />
h_\theta(x) = g(\theta^Tx)<br />
$$</p>
<p>logistic function(sigmoid function):</p>
<p>$$<br />
g(z) = \frac{1}{1+e^{-z}}<br />
$$</p>
<p>这里<br />
$$<br />
h_\theta(x)= P(y=1|x;\theta)<br />
$$<br />
含义是在x已知条件下，给定参数θ，事件<em>y=1</em>发生的概率  </p>
<p>logistic回归本质上是线性回归，只是在特征到结果的映射中加入了一层函数映射，即先把特征线性求和，然后使用函数g(z)将最为假设函数来预测。g(z)可以将连续值映射到0和1上。  </p>
<p>对g(z)的解释：将任意的输入映射到[0,1]区间上，我们在线性回归中可以得到一个预测值，再将该值映射到Sigmoid函数，这样我们就实现了由值到概率的转换，也就是分类任务。</p>
<p><strong>Note：</strong>   </p>
<blockquote>
<div class="codehilite"><pre>当y等于1时，假设函数计算出的概率应该大于0.5，即θ的转置乘以x需要大于等于0
当y等于0时，假设函数计算出的概率应该小于0.5，即θ的转置乘以x需要小于0
另外需要注意的是阈值0.5在一些情况下是可以改变的，从而获得我们所希望的特征
</pre></div>


</blockquote>
<h3 id="cost-function"><a name="user-content-cost-function" href="#cost-function" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>cost function</h3>
<p>$$<br />
J_\theta(x) = \frac{1}{m}\sum_{i=1}^m{cost(h_\theta(x^{(i)}),y^{(i)})}<br />
$$<br />
这里我们将 cost function 定义为<br />
$$<br />
cost(h_\theta(x),y) = \begin{cases}<br />
-\log(h_\theta(x)) &amp; y = 1 \\<br />
-\log(1-h_\theta(x)) &amp; y = 0<br />
\end{cases}<br />
$$<br />
例如，y = 1时 $ h_\theta(x) \rightarrow 1  $,cost = 0 表示误差很小。 此时，若 $ h_\theta(x) \rightarrow 0  $  ,$ cost \rightarrow  \infty   $表示误差很大  </p>
<h3 id="simple-classification"><a name="user-content-simple-classification" href="#simple-classification" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Simple Classification(简单分类算法)</h3>
<blockquote>
<div class="codehilite"><pre>Note：y=0 or 1
</pre></div>


</blockquote>
<p>这里对cost function进行优化，表示为：<br />
$$<br />
cost(h_\theta(x),y) = -y\log(h_\theta(x))-(1-y)\log(1-h_\theta(x))<br />
$$</p>
<blockquote>
<div class="codehilite"><pre><span class="nx">这里的cost</span> <span class="kd">function</span> <span class="nx">实际上也是由对θ的极大似然估计推导出来的</span><span class="err">。</span>
</pre></div>


</blockquote>
<p>by the way,remind:<br />
$$<br />
J_\theta(x) = \frac{1}{m}\sum_{i=1}^m{cost(h_\theta(x^{(i)}),y^{(i)})}<br />
$$</p>
<p>$$<br />
h_\theta(x) = \frac{1}{1+e^{-\theta^Tx}}<br />
$$</p>
<p>ok,接着我们对 $  J_\theta(x)  $ 计算偏微分</p>
<p>$$<br />
\begin{equation} \begin{split}<br />
\frac{\partial}{\partial\theta_j}J(\theta) &amp;=<br />
\frac{1}{m} \sum_{i=1}^m \frac{\partial}{\partial\theta_j}cost(h_\theta(x^{(i)}),y^{(i)})\\<br />
&amp;= \frac{1}{m} \sum_{i=1}^m (-y \frac{1}{h_\theta(x)} \frac{\partial}{\partial\theta_j}h_\theta(x) - (1-y)\frac{1}{1-h_\theta(x)}(-1) \frac{\partial}{\partial\theta_j}h_\theta(x) )<br />
\end{split} \end{equation}<br />
$$</p>
<p>其中 </p>
<p>$$<br />
\begin{equation} \begin{split}<br />
\frac{1}{h_\theta(x)} &amp;= 1+e^{-\theta^Tx} \\<br />
\\<br />
\frac{1}{1-h_\theta(x)} &amp;= \frac{1}{1-\frac{1}{1+e^{-\theta^Tx}}} \\<br />
&amp;= \frac{1+e^{-\theta^Tx}}{e^{-\theta^Tx}} \\<br />
&amp;= 1 + e^{\theta^Tx} \\<br />
\\<br />
\frac{\partial}{\partial\theta_j}h_\theta(x) &amp;= -(1+e^{-\theta^Tx})^{-2}(e^{-\theta^T x})(-x_j) \\<br />
&amp;= (h_\theta(x))^{2}x_je^{-\theta^T x}<br />
\end{split} \end{equation}<br />
$$<br />
将上面式子代入 $  \frac{\partial}{\partial\theta_j}J(\theta)  $ 得<br />
$$<br />
\begin{equation} \begin{split}<br />
\frac{\partial}{\partial\theta_j}J(\theta) = &amp;\frac{1}{m} \sum_{i=1}^m (-y(1+e^{-\theta^Tx}) (h_\theta(x))^{2}x_je^{-\theta^T x}   + \\<br />
&amp;(1-y) (1 + e^{\theta^Tx}) (h_\theta(x))^{2}x_je^{-\theta^T x} ) \\<br />
= &amp;\frac{1}{m} \sum_{i=1}^m -y h_\theta(x) x_je^{-\theta^T x} +<br />
(1-y) h_\theta(x) x_j \\<br />
= &amp;\frac{1}{m} \sum_{i=1}^m -y(1-h_\theta(x)) x_j +<br />
 h_\theta(x) x_j - y  h_\theta(x) x_j \\<br />
= &amp;\frac{1}{m} \sum_{i=1}^m -yx_j +<br />
 h_\theta(x) x_j \\<br />
= &amp;\frac{1}{m} \sum_{i=1}^m (h_\theta(x)-y) x_j<br />
\end{split} \end{equation}<br />
$$<br />
<strong>这里我们推出一个重要的结论</strong><br />
$$<br />
\frac{\partial}{\partial\theta_j}J(\theta) = \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)}) x_j \\<br />
\theta_j := \theta_j - \alpha \frac{\partial}{\partial\theta_j}J(\theta) = \theta_j - \alpha \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)}) x_j<br />
$$</p>
<blockquote>
<p><strong>Note:</strong><br />
  这里的$  h_\theta(x^{(i)})  $ 与 线性回归模型中的 $  h_\theta(x^{(i)})  $ 定义不一样，尽管计算出来的$  \frac{\partial}{\partial\theta_j}J(\theta)  $形式相同</p>
</blockquote>
<h3 id="code_2"><a name="user-content-code_2" href="#code_2" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Code</h3>
<div class="codehilite"><pre><span class="ch">#!/usr/bin/python</span>
<span class="c1"># -*- coding: UTF-8 -*-</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">本例程是根据学生两门课的成绩判断是否录取</span>
<span class="sd">&#39;&#39;&#39;</span>


<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="n">dataStr</span> <span class="o">=</span> <span class="s">&#39;&#39;&#39;</span>
<span class="s">34.62365962451697,78.0246928153624,0</span>
<span class="s">30.28671076822607,43.89499752400101,0</span>
<span class="s">35.84740876993872,72.90219802708364,0</span>
<span class="s">60.18259938620976,86.30855209546826,1</span>
<span class="s">79.0327360507101,75.3443764369103,1</span>
<span class="s">45.08327747668339,56.3163717815305,0</span>
<span class="s">61.10666453684766,96.51142588489624,1</span>
<span class="s">75.02474556738889,46.55401354116538,1</span>
<span class="s">76.09878670226257,87.42056971926803,1</span>
<span class="s">84.43281996120035,43.53339331072109,1</span>
<span class="s">95.86155507093572,38.22527805795094,0</span>
<span class="s">75.01365838958247,30.60326323428011,0</span>
<span class="s">82.30705337399482,76.48196330235604,1</span>
<span class="s">69.36458875970939,97.71869196188608,1</span>
<span class="s">39.53833914367223,76.03681085115882,0</span>
<span class="s">53.9710521485623,89.20735013750205,1</span>
<span class="s">69.07014406283025,52.74046973016765,1</span>
<span class="s">67.94685547711617,46.67857410673128,0</span>
<span class="s">70.66150955499435,92.92713789364831,1</span>
<span class="s">76.97878372747498,47.57596364975532,1</span>
<span class="s">67.37202754570876,42.83843832029179,0</span>
<span class="s">89.67677575072079,65.79936592745237,1</span>
<span class="s">50.534788289883,48.85581152764205,0</span>
<span class="s">34.21206097786789,44.20952859866288,0</span>
<span class="s">77.9240914545704,68.9723599933059,1</span>
<span class="s">62.27101367004632,69.95445795447587,1</span>
<span class="s">80.1901807509566,44.82162893218353,1</span>
<span class="s">93.114388797442,38.80067033713209,0</span>
<span class="s">61.83020602312595,50.25610789244621,0</span>
<span class="s">38.78580379679423,64.99568095539578,0</span>
<span class="s">61.379289447425,72.80788731317097,1</span>
<span class="s">85.40451939411645,57.05198397627122,1</span>
<span class="s">52.10797973193984,63.12762376881715,0</span>
<span class="s">52.04540476831827,69.43286012045222,1</span>
<span class="s">40.23689373545111,71.16774802184875,0</span>
<span class="s">54.63510555424817,52.21388588061123,0</span>
<span class="s">33.91550010906887,98.86943574220611,0</span>
<span class="s">64.17698887494485,80.90806058670817,1</span>
<span class="s">74.78925295941542,41.57341522824434,0</span>
<span class="s">34.1836400264419,75.2377203360134,0</span>
<span class="s">83.90239366249155,56.30804621605327,1</span>
<span class="s">51.54772026906181,46.85629026349976,0</span>
<span class="s">94.44336776917852,65.56892160559052,1</span>
<span class="s">82.36875375713919,40.61825515970618,0</span>
<span class="s">51.04775177128865,45.82270145776001,0</span>
<span class="s">62.22267576120188,52.06099194836679,0</span>
<span class="s">77.19303492601364,70.45820000180959,1</span>
<span class="s">97.77159928000232,86.7278223300282,1</span>
<span class="s">62.07306379667647,96.76882412413983,1</span>
<span class="s">91.56497449807442,88.69629254546599,1</span>
<span class="s">79.94481794066932,74.16311935043758,1</span>
<span class="s">99.2725269292572,60.99903099844988,1</span>
<span class="s">90.54671411399852,43.39060180650027,1</span>
<span class="s">34.52451385320009,60.39634245837173,0</span>
<span class="s">50.2864961189907,49.80453881323059,0</span>
<span class="s">49.58667721632031,59.80895099453265,0</span>
<span class="s">97.64563396007767,68.86157272420604,1</span>
<span class="s">32.57720016809309,95.59854761387875,0</span>
<span class="s">74.24869136721598,69.82457122657193,1</span>
<span class="s">71.79646205863379,78.45356224515052,1</span>
<span class="s">75.3956114656803,85.75993667331619,1</span>
<span class="s">35.28611281526193,47.02051394723416,0</span>
<span class="s">56.25381749711624,39.26147251058019,0</span>
<span class="s">30.05882244669796,49.59297386723685,0</span>
<span class="s">44.66826172480893,66.45008614558913,0</span>
<span class="s">66.56089447242954,41.09209807936973,0</span>
<span class="s">40.45755098375164,97.53518548909936,1</span>
<span class="s">49.07256321908844,51.88321182073966,0</span>
<span class="s">80.27957401466998,92.11606081344084,1</span>
<span class="s">66.74671856944039,60.99139402740988,1</span>
<span class="s">32.72283304060323,43.30717306430063,0</span>
<span class="s">64.0393204150601,78.03168802018232,1</span>
<span class="s">72.34649422579923,96.22759296761404,1</span>
<span class="s">60.45788573918959,73.09499809758037,1</span>
<span class="s">58.84095621726802,75.85844831279042,1</span>
<span class="s">99.82785779692128,72.36925193383885,1</span>
<span class="s">47.26426910848174,88.47586499559782,1</span>
<span class="s">50.45815980285988,75.80985952982456,1</span>
<span class="s">60.45555629271532,42.50840943572217,0</span>
<span class="s">82.22666157785568,42.71987853716458,0</span>
<span class="s">88.9138964166533,69.80378889835472,1</span>
<span class="s">94.83450672430196,45.69430680250754,1</span>
<span class="s">67.31925746917527,66.58935317747915,1</span>
<span class="s">57.23870631569862,59.51428198012956,1</span>
<span class="s">80.36675600171273,90.96014789746954,1</span>
<span class="s">68.46852178591112,85.59430710452014,1</span>
<span class="s">42.0754545384731,78.84478600148043,0</span>
<span class="s">75.47770200533905,90.42453899753964,1</span>
<span class="s">78.63542434898018,96.64742716885644,1</span>
<span class="s">52.34800398794107,60.76950525602592,0</span>
<span class="s">94.09433112516793,77.15910509073893,1</span>
<span class="s">90.44855097096364,87.50879176484702,1</span>
<span class="s">55.48216114069585,35.57070347228866,0</span>
<span class="s">74.49269241843041,84.84513684930135,1</span>
<span class="s">89.84580670720979,45.35828361091658,1</span>
<span class="s">83.48916274498238,48.38028579728175,1</span>
<span class="s">42.2617008099817,87.10385094025457,1</span>
<span class="s">99.31500880510394,68.77540947206617,1</span>
<span class="s">55.34001756003703,64.9319380069486,1</span>
<span class="s">74.77589300092767,89.52981289513276,1</span>
<span class="s">&#39;&#39;&#39;</span>

<span class="n">tmpdataList</span> <span class="o">=</span> <span class="n">dataStr</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
<span class="n">dataList</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">tmpdataList</span><span class="p">:</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">&#39;,&#39;</span><span class="p">)</span>
    <span class="n">dataList</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="k">del</span> <span class="n">tmpdataList</span>

<span class="c1"># define the prepared 训练集</span>
<span class="c1"># the meaning of column : x1,x2,y</span>
<span class="n">dataSet</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dataList</span><span class="p">)</span>
<span class="n">dataSet</span> <span class="o">=</span> <span class="n">dataSet</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">shuffleData</span><span class="p">(</span><span class="n">dataSet</span><span class="p">):</span>
    <span class="c1"># 打乱数据</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">dataSet</span><span class="p">)</span>
    <span class="n">m</span><span class="p">,</span><span class="n">n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">dataSet</span><span class="p">)</span>

    <span class="n">trainData</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">m</span><span class="p">,</span><span class="n">n</span><span class="p">))</span>
    <span class="n">trainData</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">dataSet</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="c1"># 获取dataSet的最后一列 并 强制类型转换</span>
    <span class="n">trainLabel</span> <span class="o">=</span> <span class="n">dataSet</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">trainData</span><span class="p">,</span><span class="n">trainLabel</span>


<span class="c1"># 这里我们使用matplot先看一下数据</span>
<span class="n">negativeData</span> <span class="o">=</span> <span class="n">dataSet</span><span class="p">[</span><span class="n">dataSet</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mf">0.0</span><span class="p">]</span>
<span class="n">positiveData</span> <span class="o">=</span> <span class="n">dataSet</span><span class="p">[</span><span class="n">dataSet</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">]</span>
<span class="n">trainLabel</span> <span class="o">=</span> <span class="n">dataSet</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>


<span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">positiveData</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">positiveData</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">s</span> <span class="o">=</span> <span class="mi">30</span><span class="p">,</span><span class="n">c</span> <span class="o">=</span> <span class="s">&#39;b&#39;</span><span class="p">,</span><span class="n">marker</span> <span class="o">=</span> <span class="s">&#39;o&#39;</span><span class="p">,</span><span class="n">label</span> <span class="o">=</span> <span class="s">&#39;Admited&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">negativeData</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">negativeData</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">s</span> <span class="o">=</span> <span class="mi">30</span><span class="p">,</span><span class="n">c</span> <span class="o">=</span> <span class="s">&#39;r&#39;</span><span class="p">,</span><span class="n">marker</span> <span class="o">=</span> <span class="s">&#39;x&#39;</span><span class="p">,</span><span class="n">label</span> <span class="o">=</span> <span class="s">&#39;Not Admited&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">&#39;Exam 1 Score&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">&#39;Exam 2 Score&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># 下面是逻辑回归算法</span>
<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">)))</span>


<span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">theta</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">theta</span><span class="p">))</span>

<span class="c1"># x2 x1 x0</span>
<span class="c1"># res = model(trainData,theta)</span>
<span class="k">def</span> <span class="nf">cost_function</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">theta</span><span class="p">):</span>
    <span class="n">h_x</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">theta</span><span class="p">)</span>
    <span class="n">left</span> <span class="o">=</span> <span class="o">-</span><span class="n">y</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">h_x</span><span class="p">)</span>
    <span class="n">right</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">h_x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">left</span> <span class="o">-</span> <span class="n">right</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

<span class="c1"># x = cost_function(trainData,trainLabel,theta)</span>
<span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">theta</span><span class="p">):</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">error</span> <span class="o">=</span> <span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">theta</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">ravel</span><span class="p">())):</span>
        <span class="n">term</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">error</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span><span class="n">j</span><span class="p">])</span>
        <span class="n">grad</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">term</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">grad</span>
<span class="c1"># 3种梯度下降方法  1.批处理 2.小批处理 3.随机处理</span>
<span class="c1"># 数据量较小，直接批处理即可</span>
<span class="k">def</span> <span class="nf">batchGradientDescent</span><span class="p">(</span><span class="n">dataSet</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">maxIteration</span><span class="p">,</span> <span class="n">thresh</span><span class="p">):</span>
    <span class="n">X</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="n">shuffleData</span><span class="p">(</span><span class="n">dataSet</span><span class="p">)</span>
    <span class="n">m</span><span class="p">,</span><span class="n">n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">m</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,))</span>

    <span class="n">trainX</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">maxIteration</span><span class="p">):</span>
        <span class="n">error</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span>
        <span class="n">_gradient</span> <span class="o">=</span> <span class="n">k</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">trainX</span><span class="p">,</span> <span class="n">error</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">_gradient</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">thresh</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="k">print</span><span class="p">(</span><span class="s">&#39;hit thresh1&#39;</span><span class="p">)</span>
            <span class="k">break</span>
        <span class="c1"># print(gradient(X,y,theta))</span>
        <span class="c1"># print(_gradient)</span>
        <span class="n">cost1</span> <span class="o">=</span> <span class="n">cost_function</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">theta</span><span class="p">)</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">_gradient</span>
        <span class="n">cost2</span> <span class="o">=</span> <span class="n">cost_function</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">theta</span><span class="p">)</span>
        <span class="k">if</span>  <span class="nb">abs</span><span class="p">(</span><span class="n">cost2</span> <span class="o">-</span> <span class="n">cost1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">thresh</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">&#39;hit thresh2&#39;</span><span class="p">)</span>
            <span class="k">break</span>
        <span class="c1"># print(theta)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&#39;the number of iteration is </span><span class="si">%d</span><span class="s">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="c1"># print(error)</span>
    <span class="k">return</span> <span class="n">theta</span>

<span class="c1"># theta = batchGradientDescent(dataSet,alpha =0.001,maxIteration = 1000000,thresh = (1e-6,1e-6))</span>
<span class="c1"># print(theta)</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">hit thresh2</span>
<span class="sd">the number of iteration is 109902</span>
<span class="sd">[ 0.04771429  0.04072397 -5.13364014]</span>

<span class="sd">这个数据说明当迭代次数为110000次时，cost function下降就跟缓慢了</span>
<span class="sd">&#39;&#39;&#39;</span>

<span class="n">theta</span> <span class="o">=</span> <span class="n">batchGradientDescent</span><span class="p">(</span><span class="n">dataSet</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span><span class="mf">0.001</span><span class="p">,</span><span class="n">maxIteration</span> <span class="o">=</span> <span class="mi">1000000</span><span class="p">,</span><span class="n">thresh</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.05</span><span class="p">,</span><span class="mf">1e-6</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
<span class="c1"># theta = batchGradientDescent(dataSet,alpha =0.001,maxIteration = 1000000,thresh = (1e-6,1e-6))</span>
<span class="c1"># print(theta)</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">hit thresh1</span>
<span class="sd">the number of iteration is 40046</span>
<span class="sd">[ 0.02721656  0.01899417 -2.37028409]</span>
<span class="sd">[Finished in 8.2s]</span>
<span class="sd">按照梯度下降停止大概需要40000次迭代</span>
<span class="sd">&#39;&#39;&#39;</span>
</pre></div>


<blockquote>
<p>这里实际上，如果数据经过预处理以及miniBatch后获得的数据精度比较高</p>
</blockquote>
<h3 id="advanced-optimization"><a name="user-content-advanced-optimization" href="#advanced-optimization" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Advanced optimization</h3>
<p>Optimization algorithms:</p>
<ul>
<li>Gradient descent</li>
<li>Conjugate gradient</li>
<li>BFGS</li>
<li>L-BFGS<br />
后面三种算法不需要给出学习率$  \alpha  $，且运算速度较快，但是算法较为复杂，选修。</li>
</ul>
<h3 id="_12"><a name="user-content-_12" href="#_12" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>多类别处理</h3>
<p>遇到y的取值不仅仅是0,1情况时，可以将一类与其余类化为两种模型，然后用划分两类的分类算法计算出h(x)，最后每一类都对应一个h(x)，训练出模型后，判断$  \max h_\theta(x)  $对应的类即为最后输出。</p>
<h2 id="_13"><a name="user-content-_13" href="#_13" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>关于机器学习的一些概念补充</h2>
<h3 id="_14"><a name="user-content-_14" href="#_14" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>下采样与上采样</h3>
<p>下采样，对于一个不均衡的数据，让目标值(如0和1分类)中的样本数据量相同，且以数据量少的一方的样本数量为准。<br />
上采样就是以数据量多的一方的样本数量为标准，把样本数量较少的类的样本数量生成和样本数量多的一方相同，称为上采样。</p>
<h3 id="_15"><a name="user-content-_15" href="#_15" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>交叉验证</h3>
<p>交叉验证的基本思想是把在某种意义下将原始数据(dataset)进行分组,一部分做为训练集(train set),另一部分做为验证集(validation set or test set),首先用训练集对分类器进行训练,再利用验证集来测试训练得到的模型(model),以此来做为评价分类器的性能指标。</p>
<h3 id="_16"><a name="user-content-_16" href="#_16" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>二分类模型评估方法</h3>
<p>以正例（恐怖分子）的识别为例子</p>
<p>真正例（True Positive，TP）：预测值和真实值都为1<br />
假正例（False Positive，FP）：预测值为1，真实值为0(去真)<br />
真负例（True Negative，TN）:预测值与真实值都为0<br />
假负例（False Negative，FN）：预测值为0，真实值为1(存伪)</p>
<h4 id="_17"><a name="user-content-_17" href="#_17" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>召回率（也叫查全率）</h4>
<p>$$<br />
召回率=\frac{真正例}{真正例+假负例}<br />
$$</p>
<blockquote>
<div class="codehilite"><pre>正确判为恐怖分子占实际所有恐怖分子的比例。
在某些情况中，我们也许需要以牺牲另一个指标为代价来最大化精度或者召回率。
比如检测癌症
</pre></div>


</blockquote>
<h4 id="precision"><a name="user-content-precision" href="#precision" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>精确度(precision,也叫查准率)</h4>
<p>$$<br />
精确度()=\frac{真正例}{真正例+假正例}<br />
$$</p>
<blockquote>
<div class="codehilite"><pre>在所有判为恐怖分子中，真正的恐怖分子的比例。
</pre></div>


</blockquote>
<h4 id="accuracy"><a name="user-content-accuracy" href="#accuracy" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>准确率（accuracy）</h4>
<p>$$<br />
 accuracy = \frac{TP+TN}{P+N} = \frac{TP+TN}{TP+TN+FP+FN} <br />
$$</p>
<h2 id="regularization"><a name="user-content-regularization" href="#regularization" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>正则化(Regularization)</h2>
<h3 id="underfittingoverfitting"><a name="user-content-underfittingoverfitting" href="#underfittingoverfitting" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>欠拟合(underfitting)和过拟合(overfitting)</h3>
<h3 id="how-to-addressing-overfitting"><a name="user-content-how-to-addressing-overfitting" href="#how-to-addressing-overfitting" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>How to addressing overfitting</h3>
<ol>
<li>Reduce number of featrues</li>
<li>
<p>Regularization</p>
<ul>
<li>keep all the feature,but reduce magnitude/values of feature.<br />
it works well when we have a lot of features,each of which contributs a bit to predicting y.</li>
</ul>
</li>
<li>
<p>Regularization used in linear Regression</p>
<ul>
<li>$  J(\theta) = \frac{1}{2m}[\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum_{j=1}^{n}\theta_j^2]  ​$</li>
</ul>
</li>
</ol>
<blockquote>
<div class="codehilite"><pre>λ 称为 regularization parameter
</pre></div>


<p>Note:加上$  \theta^2  $ 是一种形式，有时也可以选择加上$  |\theta|  $</p>
</blockquote>
<p>3.1 Gradient descent<br />
$$<br />
\begin{align}\begin{split}<br />
Repeat  &amp;\{ \\<br />
&amp;\theta_0 := \theta_0 -\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta (x^{(i)})-y^{(i)})x_0^{(i)} \\<br />
&amp;\theta_j := \theta_j -\alpha[\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} +\frac{\lambda}{m} \theta_j] \\<br />
&amp;(j = 1,2,,&hellip;,n) \\<br />
&amp;\}<br />
\end{split}\end{align}<br />
$$</p>
<ul>
<li>其中<br />
$$<br />
\begin{equation}\begin{split}<br />
&amp;\theta_j := \theta_j (1 - \alpha \frac{\lambda}{m}) - \alpha \frac{1}{m} \sum^m_{i=1}(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}<br />
\end{split}\end{equation}<br />
$$</li>
</ul>
<p>3.2 Normal Equation<br />
$$<br />
\theta = ( X^TX+ \lambda\begin{bmatrix}{0}&amp;{0}&amp;{\cdots}&amp;{0}\\{0}&amp;{1}&amp;{\cdots}&amp;{0}\\{\vdots}&amp;{\vdots}&amp;{\ddots}&amp;{\vdots}\\{0}&amp;{0}&amp;{\cdots}&amp;{1}\\\end{bmatrix}_{n \times n} )^{-1} X^Ty<br />
$$<br />
4. Regularization used in logistic Regression</p>
<h2 id="neural-networks"><a name="user-content-neural-networks" href="#neural-networks" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Neural networks(神经网络)</h2>
<h3 id="typeical-application"><a name="user-content-typeical-application" href="#typeical-application" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Typeical Application（应用领域）</h3>
<table>
<thead>
<tr>
<th>Example</th>
<th>Principle</th>
</tr>
</thead>
<tbody>
<tr>
<td>Ad.userinfo</td>
<td>Online Advertising(Stardard NN)</td>
</tr>
<tr>
<td>Image</td>
<td>Phototapping(CNN convoliutional nerual network)</td>
</tr>
<tr>
<td>Audio</td>
<td>Speech recognation(RNN recurrent nerual network)</td>
</tr>
<tr>
<td>Machine translation</td>
<td>RNN</td>
</tr>
<tr>
<td>Autonomous driving</td>
<td>hybrid neural network + custum nerual network</td>
</tr>
</tbody>
</table>
<h4 id="concepts"><a name="user-content-concepts" href="#concepts" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Concepts</h4>
<ul>
<li>Structured Data<ul>
<li>Data in the database(have rows and cols)</li>
<li>一般是离散的、有组织结构的</li>
</ul>
</li>
<li>Unstructured Data<ul>
<li>Audio、Image、Text</li>
<li>一般是连续的、无组织结构的</li>
</ul>
</li>
</ul>
<h3 id="_18"><a name="user-content-_18" href="#_18" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>神经元</h3>
<p><img alt="img" src="/home/thb/Documents/PyCode/machine_learning/week01/assets/673793-20151219153856802-307732621.jpg" /></p>
<h3 id="layer"><a name="user-content-layer" href="#layer" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Layer</h3>
<p>神经网络是分层的<br />
一般来说<br />
<strong>Layer 1</strong>: Input Layer<br />
<strong>Layer 2~N-1</strong>: Hidden Layer<br />
<strong>Layer N</strong>: Output Layer</p>
<p><img alt="" src="https://gss3.bdstatic.com/7Po3dSag_xI4khGkpoWK1HF6hhy/baike/c0%3Dbaike80%2C5%2C5%2C80%2C26/sign=04192a7afd36afc31a013737d27080a1/c75c10385343fbf2b547c199ba7eca8065388f24.jpg" /></p>
<p><img alt="神经元模型" src="https://upload-images.jianshu.io/upload_images/9602672-75c1a75ecbf72943.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/287/format/webp" title="神经元模型" /></p>
<p>Note:</p>
<blockquote>
<p>如果没有隐藏层，只有输入层和输出层，那么我们将这种神经网络称为“感知器”（Perceptron）<br />
  在“感知器”中，有两个层次。分别是输入层和输出层。输入层里的“输入单元”只负责传输数据，不做计算。输出层里的“输出单元”则需要对前面一层的输入进行计算。<br />
  感知器只能做简单的线性分类任务,对XOR（异或）这样的简单分类任务无法解决。</p>
</blockquote>
<p>Definations:<br />
1. $  a_i^{(j)}  $:&rdquo;activation&rdquo; of unit i in layer j<br />
2. $  \theta^{(j)}  $:matrix of weights controlling function mapping from $  layer_j  $ to $  layer_{j+1}  ​$</p>
<p>Examples:<br />
1. 输出仅有一个的神经网络<br />
$$<br />
a_1^{(2)} = g(\theta_{10}^{(1)}x_0 + \theta_{11}^{(1)}x_1 + \theta_{12}^{(1)}x_2 + \theta_{13}^{(1)}x_3 ) \\<br />
a_2^{(2)} = g(\theta_{20}^{(1)}x_0 + \theta_{21}^{(1)}x_1 + \theta_{22}^{(1)}x_2 + \theta_{23}^{(1)}x_3 ) \\<br />
a_3^{(2)} = g(\theta_{30}^{(1)}x_0 + \theta_{31}^{(1)}x_1 + \theta_{32}^{(1)}x_2 + \theta_{33}^{(1)}x_3 ) \\<br />
\\<br />
h_\theta(x) = a_1^{(3)} = g(\theta_{10}^{(2)}a_0 + \theta_{11}^{(2)}a_1 + \theta_{12}^{(2)}a_2 + \theta_{13}^{(2)}a_3 )<br />
$$</p>
<h3 id="forward-propagation"><a name="user-content-forward-propagation" href="#forward-propagation" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Forward propagation</h3>
<p>令<br />
$$<br />
z_1^{(2)} = \theta_{10}^{(1)}x_0 + \theta_{11}^{(1)}x_1 + \theta_{12}^{(1)}x_2 + \theta_{13}^{(1)}x_3 \\<br />
z_2^{(2)} = \theta_{20}^{(1)}x_0 + \theta_{21}^{(1)}x_1 + \theta_{22}^{(1)}x_2 + \theta_{23}^{(1)}x_3 \\<br />
z_3^{(2)} = \theta_{30}^{(1)}x_0 + \theta_{31}^{(1)}x_1 + \theta_{32}^{(1)}x_2 + \theta_{33}^{(1)}x_3<br />
$$<br />
则有<br />
$$<br />
a_0^{(2)} = 1 \\<br />
a_1^{(2)} = g(z_1^{(2)}) \\<br />
a_2^{(2)} = g(z_2^{(2)}) \\<br />
a_3^{(2)} = g(z_3^{(2)}) \\<br />
\\<br />
z^{(3)} = \theta^{(2)}a^{(2)} \\<br />
\\<br />
h_\theta(x) = a^{(3)} = g(z^{(3)})<br />
$$<br />
以上过程称为 <strong>Forward propagation</strong><br />
if network has $  s_j  $ uints in layer j,$  s_{j+1}  $ uints in layer j+1,then $  \theta^{(j)}  $ will be of dimension $  s_{j+1}\times(s_j+1)  $</p>
<h3 id="multi-class-classification"><a name="user-content-multi-class-classification" href="#multi-class-classification" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Multi-class classification</h3>
<p>若是表示多个输出，那么$  h_\theta(x)  $ 维度将大于1，变成一个向量矩阵，这个时候输出也就变成了多为</p>
<h3 id="cost-function_1"><a name="user-content-cost-function_1" href="#cost-function_1" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>cost function</h3>
<p>对于<br />
$$<br />
{(x^{(1)},y^{(1)}),(x^{(2)},y^{(21)}),&hellip;,(x^{(m)},y^{(m)})}<br />
$$<br />
这m个样本数据训练出来的神经网络来说，我们定义:<br />
L = total number of layers in network<br />
$  s_l  $ = no. of units(not counting bias unit) in layer $  l  $  </p>
<p>我们类比<strong>logistic regression</strong>的 $  J( (\theta)  $<br />
$$<br />
J( \theta  ) = -\frac{1}{m}[\sum_{i=1}^{m}y^{(i)} \log(h_\theta(x^{(i)})) + (1-y^{(i)})\log(1-h_\theta(x^{(i)})) ] + \frac{\lambda}{2m}\sum_{j=1}^{m}\theta_{j}^2 <br />
$$</p>
<p>Neural network:<br />
$$<br />
h_\theta(x) \in \mathbf{R}^K<br />
$$<br />
$$<br />
\lgroup h_\theta(x) \rgroup_{i} = i^{th} output<br />
$$<br />
那么在神经网络中，cost function定义为<br />
$$<br />
J(\theta) = -\frac{1}{m}[\sum_{i=1}^{m} \sum_{k=1}^{K} y_k^{(i)} \log(\lgroup h_\theta(x^{(i)}) \rgroup_k) + (1-y_k^{(i)}) \log(1- \lgroup h_\theta(x^{(i)}) \rgroup_k) ] + \\<br />
\frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{j=1}^{S_l}\sum_{j=1}^{S_{l+1}}(\theta_{ji}^{(l)})^2<br />
$$</p>
<blockquote>
<p>Note:<br />
1. l-1表示去掉输出层<br />
2. $ i= 1 \to s_l  $ 表示去掉$  \theta_{j0}  $这一列<br />
3. $ j= 1 \to s_{j+1}  $  表示全部行</p>
</blockquote>
<h3 id="backpropagetion-algorithm"><a name="user-content-backpropagetion-algorithm" href="#backpropagetion-algorithm" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Backpropagetion algorithm(反向传播算法)</h3>
<p>如果看不懂可以借鉴 <a href="https://blog.csdn.net/Chenyukuai6625/article/details/74304992">Blog链接</a></p>
<ol>
<li>Forward propagation<br />
$$<br />
\begin{align}<br />
&amp;a^{(1)} = x \\<br />
&amp;z^{(2)} = \theta^{(1)}a^{(1)} \\<br />
&amp;a^{(2)} = g(z^{(2)}) (add\ a_0^{(2)}) \\<br />
&amp;z^{(3)} = \theta^{(2)}a^{(2)} \\<br />
&amp;a^{(3)} = g(z^{(3)}) (add\ a_0^{(3)}) \\<br />
&amp;z^{(4)} = \theta^{(3)}a^{(3)} \\<br />
&amp;a^{(4)} = h_\theta(x) = g(z^{(4)})<br />
\end{align}<br />
$$</li>
<li>为了计算导数项，引入Back propagation algorithm<br />
Intuition: $ \delta^{(l)} = &ldquo;error&rdquo;\ of\ node\ j\ in\ layer\ l ​$ <br />
$$<br />
\delta^{(l)} = \frac{\delta}{\delta z_j^{(l)}} cost(i) \\<br />
eg.\ cost(i) = y^{(i)}log(h_\theta(z^{(i)}))+(1-y^{(i)})log(h_\theta(z^{(i)})) \\<br />
$$<br />
直观理解</li>
</ol>
<p><img alt="1555498198927" src="/home/thb/Documents/PyCode/machine_learning/week01/assets/1555498198927.png" /></p>
<p>Example:<br />
For each output unit(layer L = 4)<br />
$$<br />
\delta_j^{(4)} = a_j^{(4)} - y_j \ (a_j^{(4)} = h_\theta(x)_j) \\ <br />
\delta_j^{(3)} = (\theta^{(3)})^T \delta^{(4)}\ .<em>\ g&rsquo;(z^{(3)}) \\<br />
\delta_j^{(2)} = (\theta^{(2)})^T \delta^{(3)}\ .</em>\ g&rsquo;(z^{(2)}) \\<br />
g&rsquo;(z^{(3)}) = a^{(3)}\ .*\ (1-a^{(3)})<br />
$$</p>
<p>Step:<br />
Training set $ {(x^{(1)},y^{(1)}),(x^{(2)},y^{(21)}),&hellip;,(x^{(m)},y^{(m)})} ​$<br />
Set $ \Delta_{ij}^{(l)} = 0 \ (for\ all\ l,i,j) ​$ (use to compute $ \frac{\delta}{\delta\theta_{ij}^{(l)}}J(\theta) ​$)</p>
<p>For i = 1 to m<br />
set $ a^{(1)} = x^{(i)} ​$<br />
Perform forward propagation to compute a^{(i)} for l=2,3,&hellip;,L<br />
Using $ y^{(i)} ​$,compute $ \delta^(l) = a^{(l)} - y^{(i)} ​$<br />
Compute $\delta^{(L-1)},\delta^{(L-2)},&hellip;,\delta^{(2)} ​$<br />
$ \Delta_{ij}^{(l)} := \Delta_{ij}^{(l)} + a_j^{(l)}\delta_j^{(l+1)} ​$<br />
$ \Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^T ​$<br />
$ D_{ij}^{(l)} := \frac{1}{m}\Delta_{ij}^{(l)} + \lambda \theta_{ij}^{(l)} \ if j\  \ne 0 ​$<br />
$ D_{ij}^{(l)} := \frac{1}{m}\Delta_{ij}^{(l)} \ if \ j = 0 ​$<br />
$ \frac{\delta}{\delta_{ij}^{(l)}}J(\theta) = D_{ij}^{(l)} ​$</p>
<h3 id="gradient-checking"><a name="user-content-gradient-checking" href="#gradient-checking" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Gradient checking(梯度检测)</h3>
<p>原理：<br />
$$<br />
\frac{d}{d\theta}J(\theta) \approx \frac{J(\theta + \epsilon)-J(\theta-\epsilon)}{2\epsilon}(双侧差分) \\<br />
eg.\epsilon = 10^{-4}<br />
$$<br />
As for $ \vec\theta $ <br />
$ \vec\theta \in \mathbf{R}^{n} $<br />
$ \vec\theta = [\theta_1,\theta_2,&hellip;,\theta_n] ​$<br />
$$<br />
\begin{align}</p>
<p>&amp;\frac{d}{d\theta}J(\theta_1) \approx \frac{J(\theta_1+\epsilon,\theta_2,&hellip;,\theta_n)-J(\theta_1-\epsilon,\theta_2,&hellip;,\theta_n)}{2\epsilon} \\<br />
&amp;\frac{d}{d\theta}J(\theta_n) \approx \frac{J(\theta_1,\theta_2+\epsilon,&hellip;,\theta_n)-J(\theta_1,\theta_2-\epsilon,&hellip;,\theta_n)}{2\epsilon} \\<br />
&amp;\vdots \\<br />
&amp;\frac{d}{d\theta}J(\theta_n) \approx \frac{J(\theta_1,\theta_2,&hellip;,\theta_n+\epsilon)-J(\theta_1,\theta_2,&hellip;,\theta_n-\epsilon)}{2\epsilon}<br />
\end{align}<br />
$$</p>
<p>check that $ D_{vect} \approx gradApprox $<br />
gradApprox is calculated by<br />
$$<br />
\frac{d}{d\theta}J(\theta) \approx \frac{J(\theta + \epsilon)-J(\theta-\epsilon)}{2\epsilon}<br />
$$<br />
$ D_{vect} $ is calculated bt Backpropation</p>
<p>Note:</p>
<blockquote>
<ol>
<li>使用反向传播计算 $ D_{vect} $</li>
<li>使用梯度检验计算 gradApprox</li>
<li>确保 $ D_{vect} \approx gradApprox $</li>
<li>不再使用gradient checking,using backprop for learning</li>
</ol>
</blockquote>
<p>Important:</p>
<blockquote>
<div class="codehilite"><pre>Be sure to disable your gradient checking code before training your classifier.If yourun numerical gradient computation on evety iteration of gradient descent,your code will be bery slow
</pre></div>


</blockquote>
<h3 id="random-initialization"><a name="user-content-random-initialization" href="#random-initialization" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Random initialization(随机初始化)</h3>
<p>关于 $ \vec\theta $的初始化一般具有两种方案<br />
1. $ \vec\theta = \vec{0} $<br />
  - After each update,parameters corresponding to inputs going into each of two hidden units are identical<br />
2. Initial each $ \theta_{ij}^{(l)} $ to a random value in $[-\epsilon,\epsilon]$</p>
<p>显然我们选用方案2作为我们在神经网络中的theta参数的初始化方案</p>
<h3 id="summary"><a name="user-content-summary" href="#summary" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Summary</h3>
<p>Training a neural network<br />
1. Randomly initialize weights<br />
2. Implement forward propagation to get $ h_\theta(x^{(i)}) $ for any $ x^{(i)} ​$<br />
3. Implement code to compute cost function $ J(\theta) $<br />
4. Implement backprop to compute partial derivatives $ \frac{\delta}{\delta\theta_{jk}} J(\theta) $.for i = 1:m,Perform forward propagation and back propagation using example $ (x^{(i)},y^{(i)}) $,(Get activations $ a^{(l)} $ and delta terms $ \delta^{(l)} $ for l=2,3,&hellip;,L)<br />
5. Use gradient to compare $ \frac{\delta}{\delta\theta_{jk}} J(\theta) $.computed using backpropagation vs using numerical estimate of gradient of $ J(\theta) $<br />
6. Use gradient descent or advanced optimization method with back propogation to try to minimize $ J(\theta) $ as a function of parameters $ \theta $</p>
<p>Note:</p>
<blockquote>
<p>J(\theta) is non-convex function in neural network,So,we can only get a local minimum.</p>
</blockquote>
<h3 id="code_3"><a name="user-content-code_3" href="#code_3" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Code</h3>
<p>这里我们使用tensorflow来逐步构建一个简单的神经网络模型。  </p>
<h4 id="version-1"><a name="user-content-version-1" href="#version-1" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>version 1</h4>
<p>搜索 cifar-10 下载python格式的图片数据，一共有十类，这里我们使用二分类逻辑回归实现建模</p>
<div class="codehilite"><pre><span class="ch">#!/usr/bin/env python</span>
<span class="c1"># coding: utf-8</span>


<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>


<span class="n">CIFAT_DIR</span> <span class="o">=</span> <span class="s">&#39;../cifar-10-batches-py&#39;</span>
<span class="k">print</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">CIFAT_DIR</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">load_data</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;read data from data file&quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">filename</span><span class="p">),</span> <span class="s">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="c1"># data = pickle.load(f, encoding=&#39;bytes&#39;)</span>

        <span class="c1"># Python2.7代码</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">data</span><span class="p">[</span><span class="s">&#39;data&#39;</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="s">&#39;labels&#39;</span><span class="p">]</span>


<span class="k">class</span> <span class="nc">CifarData</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filenames</span><span class="p">,</span> <span class="n">need_shuffle</span><span class="p">):</span>
        <span class="n">all_data</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">all_labels</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># 关于zip函数 具体看</span>
        <span class="c1"># http://www.cnblogs.com/frydsh/archive/2012/07/10/2585370.html</span>
        <span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">filenames</span><span class="p">:</span>
            <span class="n">data</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">item</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
                <span class="c1"># label一共有是个类别 每个类别各 5000各</span>
                <span class="c1"># 使用该判断获取类别</span>
                <span class="k">if</span> <span class="n">label</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]:</span>
                    <span class="n">all_data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
                    <span class="n">all_labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
        <span class="c1"># 关于 vstack函数</span>
        <span class="c1"># https://www.cnblogs.com/nkh222/p/8932369.html</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">all_data</span><span class="p">)</span>
        <span class="c1"># 归一化处理</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">/</span> <span class="mf">127.5</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">(</span><span class="n">all_labels</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_labels</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_examples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_need_shuffle</span> <span class="o">=</span> <span class="n">need_shuffle</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_indicator</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_need_shuffle</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_shuffle_data</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_shuffle_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># 【0,1,2,3,4】 =&gt; [2,1,3,4,0]</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_examples</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_labels</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">next_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;return batch_size examples as a batch &quot;&quot;&quot;</span>
        <span class="n">end_indicator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_indicator</span> <span class="o">+</span> <span class="n">batch_size</span>
        <span class="k">if</span> <span class="n">end_indicator</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_examples</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_need_shuffle</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_shuffle_data</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_indicator</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">end_indicator</span> <span class="o">=</span> <span class="n">batch_size</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s">&quot;have no more examples&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">end_indicator</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_examples</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s">&#39;batch size is larger than all examles&#39;</span><span class="p">)</span>
        <span class="n">batch_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_indicator</span><span class="p">:</span> <span class="n">end_indicator</span><span class="p">]</span>
        <span class="n">batch_labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_labels</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_indicator</span><span class="p">:</span> <span class="n">end_indicator</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_indicator</span> <span class="o">=</span> <span class="n">end_indicator</span>
        <span class="k">return</span> <span class="n">batch_data</span><span class="p">,</span> <span class="n">batch_labels</span>


<span class="n">train_filenames</span> <span class="o">=</span> <span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">CIFAT_DIR</span><span class="p">,</span> <span class="s">&#39;data_batch_</span><span class="si">%d</span><span class="s">&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)]</span>
<span class="n">test_filenames</span> <span class="o">=</span> <span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">CIFAT_DIR</span><span class="p">,</span> <span class="s">&#39;test_batch&#39;</span><span class="p">)]</span>

<span class="n">train_data</span> <span class="o">=</span> <span class="n">CifarData</span><span class="p">(</span><span class="n">train_filenames</span><span class="p">,</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">CifarData</span><span class="p">(</span><span class="n">test_filenames</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>
<span class="c1"># batch_data, batch_labels = train_data.next_batch(10)</span>
<span class="c1"># print(batch_data,batch_labels)</span>


<span class="c1"># None 代表输入样本数是不确定的</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">3072</span><span class="p">])</span>
<span class="c1"># None</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">])</span>
<span class="c1"># 先构造一个 二分类器 因此输出为1</span>
<span class="c1"># (3072,1)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">&#39;w&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">],</span> <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal_initializer</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="c1"># (1, )</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">&#39;b&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="mf">0.0</span><span class="p">))</span>
<span class="c1"># [None,3072] *[3072,1] = [None,1]</span>
<span class="n">y_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
<span class="c1"># [None,1]</span>
<span class="n">p_y_1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">y_</span><span class="p">)</span>
<span class="c1"># 这里-1参数表示缺省值 保证为1列即可</span>
<span class="n">y_reshaped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">y_reshaped_float</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">y_reshaped</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="c1"># 计算loss</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">y_reshaped_float</span> <span class="o">-</span> <span class="n">p_y_1</span><span class="p">))</span>
<span class="n">predict</span> <span class="o">=</span> <span class="n">p_y_1</span> <span class="o">&gt;</span> <span class="mf">0.5</span>
<span class="n">correct_prediction</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">predict</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span> <span class="n">y_reshaped</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct_prediction</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">))</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">&#39;train_op&#39;</span><span class="p">):</span>
    <span class="c1"># 这里1e-3是学习率 learning rate AdamOptimizer是梯度下降的一个变种</span>
    <span class="n">train_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="mf">1e-3</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">到此为止我们的计算图搭建完成</span>
<span class="sd">&#39;&#39;&#39;</span>

<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">train_steps</span> <span class="o">=</span> <span class="mi">100000</span>
<span class="n">test_steps</span> <span class="o">=</span> <span class="mi">100</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">train_steps</span><span class="p">):</span>
        <span class="n">batch_data</span><span class="p">,</span> <span class="n">batch_labels</span> <span class="o">=</span> <span class="n">train_data</span><span class="o">.</span><span class="n">next_batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="n">loss_val</span><span class="p">,</span> <span class="n">accu_val</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
            <span class="p">[</span><span class="n">loss</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">,</span> <span class="n">train_op</span><span class="p">],</span>
            <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">batch_data</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">batch_labels</span><span class="p">})</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">500</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">&#39;[Train] Step: </span><span class="si">%d</span><span class="s">, loss: </span><span class="si">%4.5f</span><span class="s">,acc: </span><span class="si">%4.5f</span><span class="s">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">loss_val</span><span class="p">,</span> <span class="n">accu_val</span><span class="p">))</span>
        <span class="k">if</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">5000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">test_data</span> <span class="o">=</span> <span class="n">CifarData</span><span class="p">(</span><span class="n">test_filenames</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>
            <span class="n">all_test_acc_val</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">test_steps</span><span class="p">):</span>
                <span class="n">test_batch_data</span><span class="p">,</span> <span class="n">test_batch_labels</span> \
                 <span class="o">=</span> <span class="n">test_data</span><span class="o">.</span><span class="n">next_batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
                <span class="n">test_acc_val</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
                    <span class="p">[</span><span class="n">accuracy</span><span class="p">],</span>
                    <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span>
                        <span class="n">x</span><span class="p">:</span> <span class="n">test_batch_data</span><span class="p">,</span>
                        <span class="n">y</span><span class="p">:</span> <span class="n">test_batch_labels</span>
                    <span class="p">}</span>
                <span class="p">)</span>
                <span class="n">all_test_acc_val</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_acc_val</span><span class="p">)</span>
            <span class="n">test_acc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">all_test_acc_val</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="s">&#39;[Test] Step: </span><span class="si">%d</span><span class="s">, acc: </span><span class="si">%4.5f</span><span class="s"> &#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">test_acc</span><span class="p">))</span>
</pre></div>


<p>运行结果:<br />
<div class="codehilite"><pre>[Train] Step: 98500, loss: 0.10032,acc: 0.90000
[Train] Step: 99000, loss: 0.10000,acc: 0.90000
[Train] Step: 99500, loss: 0.10080,acc: 0.90000
[Train] Step: 100000, loss: 0.05529,acc: 0.95000
(2000, 3072)
(2000,)
[Test] Step: 100000, acc: 0.81200 

Process finished with exit code 0
</pre></div>
</p>
<h4 id="version-2"><a name="user-content-version-2" href="#version-2" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>version 2</h4>
<p>这里我们继续使用该算法实现多分类器</p>
<div class="codehilite"><pre><span class="ch">#!/usr/bin/env python</span>
<span class="c1"># coding: utf-8</span>


<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>


<span class="n">CIFAT_DIR</span> <span class="o">=</span> <span class="s">&#39;../cifar-10-batches-py&#39;</span>
<span class="k">print</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">CIFAT_DIR</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">load_data</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;read data from data file&quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">filename</span><span class="p">),</span> <span class="s">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="c1"># data = pickle.load(f, encoding=&#39;bytes&#39;)</span>

        <span class="c1"># Python2.7代码</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">data</span><span class="p">[</span><span class="s">&#39;data&#39;</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="s">&#39;labels&#39;</span><span class="p">]</span>


<span class="k">class</span> <span class="nc">CifarData</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filenames</span><span class="p">,</span> <span class="n">need_shuffle</span><span class="p">):</span>
        <span class="n">all_data</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">all_labels</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># 关于zip函数 具体看</span>
        <span class="c1"># http://www.cnblogs.com/frydsh/archive/2012/07/10/2585370.html</span>
        <span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">filenames</span><span class="p">:</span>
            <span class="n">data</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">item</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
                <span class="n">all_data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
                <span class="n">all_labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
        <span class="c1"># 关于 vstack函数</span>
        <span class="c1"># https://www.cnblogs.com/nkh222/p/8932369.html</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">all_data</span><span class="p">)</span>
        <span class="c1"># 归一化处理</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">/</span> <span class="mf">127.5</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">(</span><span class="n">all_labels</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_labels</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_examples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_need_shuffle</span> <span class="o">=</span> <span class="n">need_shuffle</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_indicator</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_need_shuffle</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_shuffle_data</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_shuffle_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># 【0,1,2,3,4】 =&gt; [2,1,3,4,0]</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_examples</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_labels</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">next_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;return batch_size examples as a batch &quot;&quot;&quot;</span>
        <span class="n">end_indicator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_indicator</span> <span class="o">+</span> <span class="n">batch_size</span>
        <span class="k">if</span> <span class="n">end_indicator</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_examples</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_need_shuffle</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_shuffle_data</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_indicator</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">end_indicator</span> <span class="o">=</span> <span class="n">batch_size</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s">&quot;have no more examples&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">end_indicator</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_examples</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s">&#39;batch size is larger than all examles&#39;</span><span class="p">)</span>
        <span class="n">batch_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_indicator</span><span class="p">:</span> <span class="n">end_indicator</span><span class="p">]</span>
        <span class="n">batch_labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_labels</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_indicator</span><span class="p">:</span> <span class="n">end_indicator</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_indicator</span> <span class="o">=</span> <span class="n">end_indicator</span>
        <span class="k">return</span> <span class="n">batch_data</span><span class="p">,</span> <span class="n">batch_labels</span>


<span class="n">train_filenames</span> <span class="o">=</span> <span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">CIFAT_DIR</span><span class="p">,</span> <span class="s">&#39;data_batch_</span><span class="si">%d</span><span class="s">&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)]</span>
<span class="n">test_filenames</span> <span class="o">=</span> <span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">CIFAT_DIR</span><span class="p">,</span> <span class="s">&#39;test_batch&#39;</span><span class="p">)]</span>

<span class="n">train_data</span> <span class="o">=</span> <span class="n">CifarData</span><span class="p">(</span><span class="n">train_filenames</span><span class="p">,</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">CifarData</span><span class="p">(</span><span class="n">test_filenames</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>
<span class="c1"># batch_data, batch_labels = train_data.next_batch(10)</span>
<span class="c1"># print(batch_data,batch_labels)</span>


<span class="c1"># None 代表输入样本数是不确定的</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">3072</span><span class="p">])</span>
<span class="c1"># None</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">])</span>
<span class="c1"># 先构造一个 二分类器 因此输出为1</span>
<span class="c1"># (3072,10)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">&#39;w&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">10</span><span class="p">],</span> <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal_initializer</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="c1"># (10, )</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">&#39;b&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">10</span><span class="p">],</span> <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="mf">0.0</span><span class="p">))</span>
<span class="c1"># [None,3072] *[3072,10] = [None,10]</span>
<span class="n">y_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>

<span class="c1"># 关于softmax https://www.zhihu.com/question/23765351</span>
<span class="c1"># [[0,01,0.9,...,0.02],[]]</span>
<span class="n">p_y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">y_</span><span class="p">)</span>
<span class="c1"># 6 --&gt;[0,0,0,0,0,1,0,0,0,0]</span>
<span class="n">y_one_hot</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">y_one_hot</span> <span class="o">-</span> <span class="n">p_y</span><span class="p">))</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd"># [None,10]</span>
<span class="sd">p_y_1 = tf.nn.sigmoid(y_)</span>
<span class="sd"># 这里-1参数表示缺省值 保证为1列即可</span>
<span class="sd">y_reshaped = tf.reshape(y, (-1, 1))</span>
<span class="sd">y_reshaped_float = tf.cast(y_reshaped, tf.float32)</span>
<span class="sd"># 计算loss</span>
<span class="sd">loss = tf.reduce_mean(tf.square(y_reshaped_float - p_y_1))</span>
<span class="sd">&#39;&#39;&#39;</span>

<span class="c1"># indices</span>
<span class="n">predict</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">correct_prediction</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">predict</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct_prediction</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">))</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">&#39;train_op&#39;</span><span class="p">):</span>
    <span class="c1"># 这里1e-3是学习率 learning rate AdamOptimizer是梯度下降的一个变种</span>
    <span class="n">train_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="mf">1e-3</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">到此为止我们的计算图搭建完成</span>
<span class="sd">&#39;&#39;&#39;</span>

<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">train_steps</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">test_steps</span> <span class="o">=</span> <span class="mi">100</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">train_steps</span><span class="p">):</span>
        <span class="n">batch_data</span><span class="p">,</span> <span class="n">batch_labels</span> <span class="o">=</span> <span class="n">train_data</span><span class="o">.</span><span class="n">next_batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="n">loss_val</span><span class="p">,</span> <span class="n">accu_val</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
            <span class="p">[</span><span class="n">loss</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">,</span> <span class="n">train_op</span><span class="p">],</span>
            <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">batch_data</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">batch_labels</span><span class="p">})</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">500</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">&#39;[Train] Step: </span><span class="si">%d</span><span class="s">, loss: </span><span class="si">%4.5f</span><span class="s">,acc: </span><span class="si">%4.5f</span><span class="s">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">loss_val</span><span class="p">,</span> <span class="n">accu_val</span><span class="p">))</span>
        <span class="k">if</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">5000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">test_data</span> <span class="o">=</span> <span class="n">CifarData</span><span class="p">(</span><span class="n">test_filenames</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>
            <span class="n">all_test_acc_val</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">test_steps</span><span class="p">):</span>
                <span class="n">test_batch_data</span><span class="p">,</span> <span class="n">test_batch_labels</span> \
                 <span class="o">=</span> <span class="n">test_data</span><span class="o">.</span><span class="n">next_batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
                <span class="n">test_acc_val</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
                    <span class="p">[</span><span class="n">accuracy</span><span class="p">],</span>
                    <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span>
                        <span class="n">x</span><span class="p">:</span> <span class="n">test_batch_data</span><span class="p">,</span>
                        <span class="n">y</span><span class="p">:</span> <span class="n">test_batch_labels</span>
                    <span class="p">}</span>
                <span class="p">)</span>
                <span class="n">all_test_acc_val</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_acc_val</span><span class="p">)</span>
            <span class="n">test_acc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">all_test_acc_val</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="s">&#39;[Test] Step: </span><span class="si">%d</span><span class="s">, acc: </span><span class="si">%4.5f</span><span class="s"> &#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">test_acc</span><span class="p">))</span>
</pre></div>


<h4 id="note_1"><a name="user-content-note_1" href="#note_1" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Note</h4>
<p>这两部分代码都没有用到hidden layer.<br />
实际上，code 1 展示的是一个神经元，这里也可以认为是逻辑回归。也就是logistic regression 看做是仅仅含有一个神经元的单 层神经网络<br />
code 2 实际上也就是多维的logistic regreesion,其实softmax regression可以看做是含有k个神经元的一层神经网络。</p></article></body></html>